{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# UAV Detection Using YOLO: Synthetic Data Training, Testing, and Result Visualization"
      ],
      "metadata": {
        "id": "MUIxOHRjLAat"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjpPg4mGKc1v",
        "outputId": "3b7fab91-4566-4943-83a2-36d030cbd54c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(HOME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8Epf5rhnpV_"
      },
      "source": [
        "## Download dataset from Roboflow Universe"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "secret = userdata.get('roboflowKey')"
      ],
      "metadata": {
        "id": "7SHIWvKLDio8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSd93ZJzZZKt"
      },
      "outputs": [],
      "source": [
        "!mkdir {HOME}/datasets\n",
        "%cd {HOME}/datasets\n",
        "\n",
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=secret)\n",
        "project = rf.workspace(\"ai-jbsna\").project(\"drone3-c8zgs\")\n",
        "version = project.version(18)\n",
        "dataset = version.download(\"yolov11\")\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir {HOME}/datasets\n",
        "%cd {HOME}/datasets\n",
        "\n",
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=secret)\n",
        "project = rf.workspace(\"ai-jbsna\").project(\"dronereal-3\")\n",
        "version = project.version(1)\n",
        "dataset = version.download(\"yolov8\")"
      ],
      "metadata": {
        "id": "LlKzOzu6K2Es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvRoruMguOIZ"
      },
      "source": [
        "**NOTE:**\n",
        "Make sure the last 4 lines of the data.yaml file have the following format:\n",
        "\n",
        "```\n",
        "test: ../test/images\n",
        "train: ../train/images\n",
        "val: ../valid/images\n",
        "```\n",
        "\n",
        "If using a dataset from Roboflow, run the command below. 👇🏻"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2LLYQIS0tbC1"
      },
      "outputs": [],
      "source": [
        "!sed -i '$d' {dataset.location}/data.yaml\n",
        "!sed -i '$d' {dataset.location}/data.yaml\n",
        "!sed -i '$d' {dataset.location}/data.yaml\n",
        "!sed -i '$d' {dataset.location}/data.yaml\n",
        "!echo -e \"test: ../test/images\\ntrain: ../train/images\\nval: ../valid/images\" >> {dataset.location}/data.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download YOLO model"
      ],
      "metadata": {
        "id": "VG1MNJyJ7ygP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ultralytics"
      ],
      "metadata": {
        "id": "Qb4U6DFsyMfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load a COCO-pretrained YOLO11n model\n",
        "model = YOLO(\"yolo11m.pt\")"
      ],
      "metadata": {
        "id": "veNDMcYDCy4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training YOLO model"
      ],
      "metadata": {
        "id": "pI_qRPay8GA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!yolo task=detect mode=train \\\n",
        "    model=/content/yolo11m.pt \\\n",
        "    data=/content/Drone3-18/data.yaml\\\n",
        "    epochs=200 \\\n",
        "    batch=16 \\\n",
        "    patience=50 \\\n",
        "    optimizer='AdamW' \\\n",
        "    lr0=0.01 \\\n",
        "    lrf=0.0001 \\\n",
        "    momentum=0.937 \\\n",
        "    weight_decay=0.0005 \\\n",
        "    warmup_epochs=3 \\\n",
        "    hsv_h=0.015 \\\n",
        "    hsv_s=0.8 \\\n",
        "    hsv_v=0.5 \\\n",
        "    degrees=15 \\\n",
        "    translate=0.2 \\\n",
        "    scale=0.6 \\\n",
        "    fliplr=0.5 \\\n",
        "    mosaic=1.0 \\\n",
        "    mixup=0.3 \\\n",
        "    copy_paste=0.3 \\\n",
        "    close_mosaic=10 \\\n",
        "    box=7.5 \\\n",
        "    cls=0.5 \\\n",
        "    dfl=1.5 \\\n",
        "    overlap_mask=True \\\n",
        "    mask_ratio=4 \\\n",
        "    seed=42"
      ],
      "metadata": {
        "id": "jiiR3HeeZgBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download YOLO folder with results"
      ],
      "metadata": {
        "id": "diFlI55U8OBp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Define the source folder and the zip file path\n",
        "folder_path = '/content/runs/detect/train'\n",
        "zip_path = '/content/yolo2002.zip'\n",
        "\n",
        "# Compress the folder into a zip file\n",
        "shutil.make_archive(zip_path.replace('.zip', ''), 'zip', folder_path)\n",
        "\n",
        "# Download the zip file to your laptop\n",
        "files.download(zip_path)"
      ],
      "metadata": {
        "id": "GiljwT3-ph3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize model results on test images"
      ],
      "metadata": {
        "id": "NsTw4fk58bQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import yaml\n",
        "import torch\n",
        "from ultralytics import YOLO\n",
        "\n",
        "def load_class_names(yaml_path):\n",
        "    \"\"\"Load class names from YAML file\"\"\"\n",
        "    with open(yaml_path, 'r') as f:\n",
        "        data = yaml.safe_load(f)\n",
        "        return data['names']\n",
        "\n",
        "def plot_predictions_with_ground_truth(model, dataset, num_images=6):\n",
        "    \"\"\"\n",
        "    Plot predictions and ground truth boxes for multiple images\n",
        "    Args:\n",
        "        model: YOLO model\n",
        "        dataset: Dictionary containing paths\n",
        "        num_images: Number of images to visualize\n",
        "    \"\"\"\n",
        "    # Load class names\n",
        "    class_names = load_class_names(dataset['data_yaml_path'])\n",
        "    # Get list of image files\n",
        "    image_files = list(Path(dataset['images_directory_path']).glob('*.jpg'))\n",
        "    np.random.shuffle(image_files)\n",
        "    # Create subplot grid\n",
        "    fig, axes = plt.subplots(3, 2, figsize=(15, 10))\n",
        "    axes = axes.ravel()\n",
        "    for idx, img_path in enumerate(image_files[:num_images]):\n",
        "        # Read image\n",
        "        img = cv2.imread(str(img_path))\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        # Get ground truth labels\n",
        "        label_path = Path(dataset['annotations_directory_path']) / f\"{img_path.stem}.txt\"\n",
        "        gt_boxes = []\n",
        "        gt_classes = []\n",
        "        if label_path.exists():\n",
        "            with open(label_path, 'r') as f:\n",
        "                for line in f:\n",
        "                    class_id, x, y, w, h = map(float, line.strip().split())\n",
        "                    gt_boxes.append([x, y, w, h])\n",
        "                    gt_classes.append(int(class_id))\n",
        "        # Get predictions\n",
        "        results = model.predict(img, conf=0.25)[0]\n",
        "        pred_boxes = results.boxes.xyxyn.cpu().numpy()  # normalized coordinates\n",
        "        pred_classes = results.boxes.cls.cpu().numpy()\n",
        "        pred_conf = results.boxes.conf.cpu().numpy()\n",
        "        # Plot image\n",
        "        axes[idx].imshow(img)\n",
        "        # Initialize lists for legend handles and labels\n",
        "        gt_handles = []\n",
        "        pred_handles = []\n",
        "        # Plot ground truth boxes\n",
        "        for box, cls_id in zip(gt_boxes, gt_classes):\n",
        "            x, y, w, h = box\n",
        "            x1, y1 = x - w/2, y - h/2\n",
        "            x2, y2 = x + w/2, y + h/2\n",
        "            rect = plt.Rectangle((x1 * img.shape[1], y1 * img.shape[0]),\n",
        "                               w * img.shape[1], h * img.shape[0],\n",
        "                               fill=False, color='green', linewidth=2,\n",
        "                               label=f'GT: {class_names[cls_id]}')\n",
        "            axes[idx].add_patch(rect)\n",
        "            gt_handles.append(rect)\n",
        "        # Plot predicted boxes\n",
        "        for box, cls_id, conf in zip(pred_boxes, pred_classes, pred_conf):\n",
        "            x1, y1, x2, y2 = box\n",
        "            rect = plt.Rectangle((x1 * img.shape[1], y1 * img.shape[0]),\n",
        "                               (x2-x1) * img.shape[1], (y2-y1) * img.shape[0],\n",
        "                               fill=False, color='red', linewidth=2,\n",
        "                               label=f'Pred: {class_names[int(cls_id)]} ({conf:.2f})')\n",
        "            axes[idx].add_patch(rect)\n",
        "            pred_handles.append(rect)\n",
        "        axes[idx].set_title(f'Image {idx+1}')\n",
        "        axes[idx].axis('off')\n",
        "        # Add legend inside the image\n",
        "        handles = gt_handles + pred_handles\n",
        "        labels = [h.get_label() for h in handles]\n",
        "        unique_labels = dict(zip(labels, handles))\n",
        "        # Position the legend in the top-right corner with a white background and smaller font\n",
        "        axes[idx].legend(unique_labels.values(), unique_labels.keys(),\n",
        "                         loc='upper right',\n",
        "                         bbox_to_anchor=(0.98, 0.98),\n",
        "                         frameon=True,\n",
        "                         facecolor='white',\n",
        "                         edgecolor='lightgray',\n",
        "                         prop={'size': 6})  # Reduced font size\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "dataset = {\n",
        "    'images_directory_path': \"/content/datasets/Drone3-18/test/images\",\n",
        "    'annotations_directory_path': \"/content/datasets/Drone3-18/test/labels\",\n",
        "    'data_yaml_path': \"/content/datasets/Drone3-18/data.yaml\"\n",
        "}\n",
        "\n",
        "# Load the model\n",
        "model = YOLO('content/YOLO200.pt')\n",
        "\n",
        "# Plot predictions with ground truth\n",
        "plot_predictions_with_ground_truth(model, dataset)\n"
      ],
      "metadata": {
        "id": "KjP6RGEyKGtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize model results on real images"
      ],
      "metadata": {
        "id": "p5kqGCc88rWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "dataset = {\n",
        "    'images_directory_path': \"/content/datasets/DroneReal-2-4/test/images\",\n",
        "    'annotations_directory_path': \"/content/datasets/DroneReal-2-4/test/labels\",\n",
        "    'data_yaml_path': \"/content/datasets/DroneReal-2-4/data.yaml\"\n",
        "}\n",
        "\n",
        "# Load the model\n",
        "model = YOLO('/content/YOLO200.pt')\n",
        "\n",
        "# Plot predictions with ground truth\n",
        "plot_predictions_with_ground_truth(model, dataset)"
      ],
      "metadata": {
        "id": "RQCf-P8p9Ary"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing YOLO model on test images"
      ],
      "metadata": {
        "id": "coKW-b068ucM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import supervision as sv\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "\n",
        "def calculate_iou(box1, box2):\n",
        "    \"\"\"\n",
        "    Calculate IoU between two boxes\n",
        "\n",
        "    Args:\n",
        "        box1: Array of shape (N, 4) containing N boxes in xyxy format\n",
        "        box2: Array of shape (M, 4) containing M boxes in xyxy format\n",
        "\n",
        "    Returns:\n",
        "        IoU matrix of shape (N, M)\n",
        "    \"\"\"\n",
        "    # Get coordinates\n",
        "    b1_x1, b1_y1, b1_x2, b1_y2 = np.split(box1, 4, axis=1)\n",
        "    b2_x1, b2_y1, b2_x2, b2_y2 = np.split(box2, 4, axis=1)\n",
        "\n",
        "    # Calculate intersection coordinates\n",
        "    x1 = np.maximum(b1_x1, np.transpose(b2_x1))\n",
        "    y1 = np.maximum(b1_y1, np.transpose(b2_y1))\n",
        "    x2 = np.minimum(b1_x2, np.transpose(b2_x2))\n",
        "    y2 = np.minimum(b1_y2, np.transpose(b2_y2))\n",
        "\n",
        "    # Calculate intersection area\n",
        "    intersection = np.maximum(0, x2 - x1) * np.maximum(0, y2 - y1)\n",
        "\n",
        "    # Calculate union area\n",
        "    b1_area = (b1_x2 - b1_x1) * (b1_y2 - b1_y1)\n",
        "    b2_area = (b2_x2 - b2_x1) * (b2_y2 - b2_y1)\n",
        "    union = b1_area + np.transpose(b2_area) - intersection\n",
        "\n",
        "    # Calculate IoU\n",
        "    iou = intersection / (union + 1e-7)\n",
        "\n",
        "    return iou\n",
        "\n",
        "def calculate_ap50(confidences, labels, valid_labels):\n",
        "    \"\"\"\n",
        "    Calculate Average Precision at IoU 0.50 for a single class\n",
        "\n",
        "    Args:\n",
        "        confidences (np.array): Confidence scores of predictions\n",
        "        labels (np.array): Binary labels (1 for true positive, 0 for false positive)\n",
        "        valid_labels (int): Number of ground truth instances\n",
        "\n",
        "    Returns:\n",
        "        float: Average Precision (AP50)\n",
        "    \"\"\"\n",
        "    # Sort predictions by confidence in descending order\n",
        "    sorted_indices = np.argsort(confidences)[::-1]\n",
        "    sorted_labels = labels[sorted_indices]\n",
        "\n",
        "    # Compute precision and recall\n",
        "    tp = np.cumsum(sorted_labels)\n",
        "    fp = np.cumsum(1 - sorted_labels)\n",
        "\n",
        "    precision = tp / (tp + fp)\n",
        "    recall = tp / max(valid_labels, 1)\n",
        "\n",
        "    # Compute AP using 11-point interpolation\n",
        "    ap = 0.0\n",
        "    for t in np.linspace(0, 1, 11):\n",
        "        # Find max precision for recalls >= t\n",
        "        precisions_at_t = precision[recall >= t]\n",
        "        max_precision = precisions_at_t.max() if len(precisions_at_t) > 0 else 0\n",
        "        ap += max_precision / 11\n",
        "\n",
        "    return ap\n",
        "\n",
        "def evaluate_yolo_model(model_path, dataset, conf_threshold=0.25, iou_threshold=0.5):\n",
        "    \"\"\"\n",
        "    Evaluate YOLO model performance on a dataset using multiple metrics\n",
        "\n",
        "    Args:\n",
        "        model_path (str): Path to the YOLO model weights\n",
        "        dataset (sv.DetectionDataset): Supervision dataset object\n",
        "        conf_threshold (float): Confidence threshold for predictions\n",
        "        iou_threshold (float): IoU threshold for match determination\n",
        "    \"\"\"\n",
        "    # Load model\n",
        "    model = YOLO(model_path)\n",
        "\n",
        "    # Initialize metrics storage\n",
        "    metrics = {\n",
        "        'precisions': [],\n",
        "        'recalls': [],\n",
        "        'f1_scores': [],\n",
        "        'confidences': [],\n",
        "        'true_positives': 0,\n",
        "        'false_positives': 0,\n",
        "        'false_negatives': 0,\n",
        "        'predictions_by_class': defaultdict(list),\n",
        "        'prediction_confidences_by_class': defaultdict(list),\n",
        "        'ground_truth_by_class': defaultdict(int)\n",
        "    }\n",
        "\n",
        "    # Evaluate each image\n",
        "    for image_name in tqdm(dataset.images):\n",
        "        # Get ground truth\n",
        "        gt_annotations = dataset.annotations[image_name]\n",
        "        gt_boxes = gt_annotations.xyxy\n",
        "        gt_classes = gt_annotations.class_id\n",
        "\n",
        "        # Update ground truth class counts\n",
        "        for cls in gt_classes:\n",
        "            metrics['ground_truth_by_class'][cls] += 1\n",
        "\n",
        "        # Get predictions\n",
        "        image = dataset.images[image_name]\n",
        "        results = model(image)[0]\n",
        "        pred_boxes = results.boxes.xyxy.cpu().numpy()\n",
        "        pred_classes = results.boxes.cls.cpu().numpy()\n",
        "        pred_conf = results.boxes.conf.cpu().numpy()\n",
        "\n",
        "        # Filter by confidence threshold\n",
        "        mask = pred_conf >= conf_threshold\n",
        "        pred_boxes = pred_boxes[mask]\n",
        "        pred_classes = pred_classes[mask]\n",
        "        pred_conf = pred_conf[mask]\n",
        "\n",
        "        # Calculate IoU matrix\n",
        "        if len(pred_boxes) > 0 and len(gt_boxes) > 0:\n",
        "            iou_matrix = calculate_iou(pred_boxes, gt_boxes)\n",
        "\n",
        "            # Match predictions to ground truth\n",
        "            matched_indices = []\n",
        "            for pred_idx, pred_class in enumerate(pred_classes):\n",
        "                best_iou = 0\n",
        "                best_gt_idx = -1\n",
        "\n",
        "                for gt_idx, gt_class in enumerate(gt_classes):\n",
        "                    if gt_idx not in matched_indices and pred_class == gt_class:\n",
        "                        iou = iou_matrix[pred_idx, gt_idx]\n",
        "                        if iou >= iou_threshold and iou > best_iou:\n",
        "                            best_iou = iou\n",
        "                            best_gt_idx = gt_idx\n",
        "\n",
        "                if best_gt_idx >= 0:\n",
        "                    matched_indices.append(best_gt_idx)\n",
        "                    metrics['true_positives'] += 1\n",
        "                    metrics['confidences'].append(pred_conf[pred_idx])\n",
        "                    metrics['predictions_by_class'][pred_class].append(True)\n",
        "                    metrics['prediction_confidences_by_class'][pred_class].append(pred_conf[pred_idx])\n",
        "                else:\n",
        "                    metrics['false_positives'] += 1\n",
        "                    metrics['predictions_by_class'][pred_class].append(False)\n",
        "                    metrics['prediction_confidences_by_class'][pred_class].append(pred_conf[pred_idx])\n",
        "\n",
        "            metrics['false_negatives'] += len(gt_boxes) - len(matched_indices)\n",
        "        else:\n",
        "            metrics['false_positives'] += len(pred_boxes)\n",
        "            metrics['false_negatives'] += len(gt_boxes)\n",
        "\n",
        "    # Calculate overall metrics\n",
        "    total_predictions = metrics['true_positives'] + metrics['false_positives']\n",
        "    total_ground_truth = metrics['true_positives'] + metrics['false_negatives']\n",
        "\n",
        "    precision = metrics['true_positives'] / total_predictions if total_predictions > 0 else 0\n",
        "    recall = metrics['true_positives'] / total_ground_truth if total_ground_truth > 0 else 0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    # Calculate per-class metrics\n",
        "    class_metrics = {}\n",
        "    for class_id in metrics['ground_truth_by_class'].keys():\n",
        "        predictions = metrics['predictions_by_class'][class_id]\n",
        "        ground_truth = metrics['ground_truth_by_class'][class_id]\n",
        "        prediction_confidences = np.array(metrics['prediction_confidences_by_class'][class_id])\n",
        "        prediction_labels = np.array(predictions).astype(int)\n",
        "\n",
        "        if len(predictions) > 0:\n",
        "            class_tp = sum(predictions)\n",
        "            class_fp = len(predictions) - class_tp\n",
        "            class_fn = ground_truth - class_tp\n",
        "\n",
        "            class_precision = class_tp / (class_tp + class_fp) if (class_tp + class_fp) > 0 else 0\n",
        "            class_recall = class_tp / (class_tp + class_fn) if (class_tp + class_fn) > 0 else 0\n",
        "            class_f1 = 2 * (class_precision * class_recall) / (class_precision + class_recall) if (class_precision + class_recall) > 0 else 0\n",
        "\n",
        "            # Calculate AP50 for the class\n",
        "            class_ap50 = calculate_ap50(prediction_confidences, prediction_labels, ground_truth)\n",
        "\n",
        "            class_metrics[class_id] = {\n",
        "                'precision': class_precision,\n",
        "                'recall': class_recall,\n",
        "                'f1': class_f1,\n",
        "                'ap50': class_ap50,\n",
        "                'support': ground_truth\n",
        "            }\n",
        "\n",
        "    # Visualize results\n",
        "    plt.figure(figsize=(20, 5))\n",
        "\n",
        "    # Plot 1: Confidence Distribution\n",
        "    plt.subplot(141)\n",
        "    if metrics['confidences']:\n",
        "        sns.histplot(metrics['confidences'], bins=20)\n",
        "        plt.title('Confidence Distribution\\nfor True Positives')\n",
        "        plt.xlabel('Confidence Score')\n",
        "        plt.ylabel('Count')\n",
        "\n",
        "    # Plot 2: Overall Metrics\n",
        "    plt.subplot(142)\n",
        "    overall_metrics = [precision, recall, f1]\n",
        "    plt.bar(['Precision', 'Recall', 'F1-Score'], overall_metrics)\n",
        "    plt.title('Overall Model Performance')\n",
        "    plt.ylim(0, 1)\n",
        "\n",
        "    # Plot 3: Per-class F1 Scores\n",
        "    plt.subplot(143)\n",
        "    class_f1_scores = [metrics['f1'] for metrics in class_metrics.values()]\n",
        "    plt.bar(list(class_metrics.keys()), class_f1_scores)\n",
        "    plt.title('F1-Score by Class')\n",
        "    plt.xlabel('Class ID')\n",
        "    plt.ylabel('F1-Score')\n",
        "    plt.ylim(0, 1)\n",
        "\n",
        "    # Plot 4: Per-class AP50\n",
        "    plt.subplot(144)\n",
        "    class_ap50_scores = [metrics['ap50'] for metrics in class_metrics.values()]\n",
        "    plt.bar(list(class_metrics.keys()), class_ap50_scores)\n",
        "    plt.title('AP50 by Class')\n",
        "    plt.xlabel('Class ID')\n",
        "    plt.ylabel('AP50')\n",
        "    plt.ylim(0, 1)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print detailed metrics\n",
        "    print(\"\\nOverall Metrics:\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1-Score: {f1:.4f}\")\n",
        "    print(f\"True Positives: {metrics['true_positives']}\")\n",
        "    print(f\"False Positives: {metrics['false_positives']}\")\n",
        "    print(f\"False Negatives: {metrics['false_negatives']}\")\n",
        "\n",
        "    print(\"\\nPer-class Metrics:\")\n",
        "    for class_id, metrics in class_metrics.items():\n",
        "        print(f\"\\nClass {class_id}:\")\n",
        "        print(f\"Precision: {metrics['precision']:.4f}\")\n",
        "        print(f\"Recall: {metrics['recall']:.4f}\")\n",
        "        print(f\"F1-Score: {metrics['f1']:.4f}\")\n",
        "        print(f\"AP50: {metrics['ap50']:.4f}\")\n",
        "        print(f\"Support: {metrics['support']}\")\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Example usage\n",
        "HOME = \"/content\"  # Adjust this to your environment\n",
        "model_path = \"/content/YOLO200.pt\"\n",
        "\n",
        "# Create dataset\n",
        "dataset = sv.DetectionDataset.from_yolo(\n",
        "    images_directory_path=\"/content/datasets/Drone3-18/test/images\",\n",
        "    annotations_directory_path=\"/content/datasets/Drone3-18/test/labels\",\n",
        "    data_yaml_path=\"/content/datasets/Drone3-18/data.yaml\"\n",
        ")\n",
        "\n",
        "# Evaluate model\n",
        "metrics = evaluate_yolo_model(model_path, dataset)"
      ],
      "metadata": {
        "id": "xT2g0lqEKee6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing YOLO model on real images"
      ],
      "metadata": {
        "id": "seQH7yor9TWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sv.DetectionDataset.from_yolo(\n",
        "    images_directory_path=\"/content/datasets/DroneReal-2-4/test/images\",\n",
        "    annotations_directory_path=\"/content/datasets/DroneReal-2-4/test/labels\",\n",
        "    data_yaml_path=\"/content/datasets/DroneReal-2-4/data.yaml\"\n",
        ")\n",
        "\n",
        "# Evaluate model\n",
        "metrics = evaluate_yolo_model(model_path, dataset)"
      ],
      "metadata": {
        "id": "9VAaCo6r9HaW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}